{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e9e30f3-1407-43f2-a5a3-059212cf4df2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils import data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import scipy\n",
    "import ot\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12bd051b-1639-4952-802b-7ba0a5ee673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, clean_data_s, noise_data_s, clean_data_t, noise_data_t):\n",
    "        self.clean_data_s = clean_data_s\n",
    "        self.noise_data_s = noise_data_s\n",
    "        self.clean_data_t = clean_data_t\n",
    "        self.noise_data_t = noise_data_t\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.clean_data_s)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        clean_sample_s = self.clean_data_s[index]\n",
    "        noise_sample_s = self.noise_data_s[index]\n",
    "        clean_sample_t = self.clean_data_t[index]\n",
    "        noise_sample_t = self.noise_data_t[index]\n",
    "        return clean_sample_s, noise_sample_s, clean_sample_t, noise_sample_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf56fb7b-2faa-4aa0-815b-a98c1d9a9054",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = torch.load('data/DATASET/TRAIN_ST_FINAL_01.pt')\n",
    "ds_test = torch.load('data/DATASET/TEST_ST_FINAL_01.pt')\n",
    "ds_val = torch.load('data/DATASET/VALIDATION_ST_FINAL_01.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a0d1968-28d8-4352-8f3d-ab91f905831d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8ef0219d50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "959552b1-47e6-4a87-b58a-81fedd81c036",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train\n",
    "custom_dataloader_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True)\n",
    "#Test\n",
    "custom_dataloader_test = DataLoader(ds_test, batch_size=batch_size, shuffle=True)\n",
    "#Val\n",
    "custom_dataloader_val = DataLoader(ds_val, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fe5b329-fd38-4c8b-84ec-4c23af21b671",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "1\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "2\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "3\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "4\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "5\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "6\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "7\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "8\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "9\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "10\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "11\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "12\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "13\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "14\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "15\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "16\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "17\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "18\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "19\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "20\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "21\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "22\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "23\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "24\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "25\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "26\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "27\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "28\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "29\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "30\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "31\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "32\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "33\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "34\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "35\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "36\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "37\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "38\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "39\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "40\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "41\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "42\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "43\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "44\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "45\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "46\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "47\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "48\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "49\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "50\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "51\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "52\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "53\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "54\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "55\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "56\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "57\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "58\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "59\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "60\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "61\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "62\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "63\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "64\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "65\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "66\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "67\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "68\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "69\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "70\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "71\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "72\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "73\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "74\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "75\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "76\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "77\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "78\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "79\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "80\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "81\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "82\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "83\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "84\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "85\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "86\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "87\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "88\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "89\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "90\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "91\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "92\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "93\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "94\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "95\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "96\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "97\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "98\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "99\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "100\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "101\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "102\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "103\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "104\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "105\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "106\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "107\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "108\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "109\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "110\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "111\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "112\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "113\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "114\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "115\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "116\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "117\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "118\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "119\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "120\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "121\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "122\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "123\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "124\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "125\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "126\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "127\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "128\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "129\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "130\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "131\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "132\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "133\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "134\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "135\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "136\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "137\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "138\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "139\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "140\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "141\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "142\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "143\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "144\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "145\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "146\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "147\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "148\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "149\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "150\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "151\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "152\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "153\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "154\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "155\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "156\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "157\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "158\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "159\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "160\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "161\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "162\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "163\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "164\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "165\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "166\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "167\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "168\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "169\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "170\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "171\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "172\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "173\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "174\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "175\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "176\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "177\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "178\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "179\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "180\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "181\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "182\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "183\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "184\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "185\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "186\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "187\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "188\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "189\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "190\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "191\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "192\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "193\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "194\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "195\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "196\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "197\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "198\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "199\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "200\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "201\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "202\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "203\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "204\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "205\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "206\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "207\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "208\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "209\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "210\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "211\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "212\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "213\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "214\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "215\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "216\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "217\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "218\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "219\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "220\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "221\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "222\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "223\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "224\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "225\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "226\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "227\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "228\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "229\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "230\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "231\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "232\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "233\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "234\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "235\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "236\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "torch.Size([128, 64, 257])\n",
      "237\n",
      "torch.Size([93, 64, 257])\n",
      "torch.Size([93, 64, 257])\n",
      "torch.Size([93, 64, 257])\n",
      "torch.Size([93, 64, 257])\n"
     ]
    }
   ],
   "source": [
    "for i, (clean_spec_s, noise_spec_s, clean_spec_t, noise_spec_t) in enumerate(custom_dataloader_train):\n",
    "    print(i)\n",
    "    print(clean_spec_s.size())\n",
    "    print(noise_spec_s.size())\n",
    "    print(clean_spec_t.size())\n",
    "    print(noise_spec_t.size())\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d3f6952-62f2-41ef-b556-133ce6de8c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb3aea52-a202-4d57-8040-53fda30da9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OT_loss(X_s, X_t, y_s, y_t_pred):\n",
    "    N = X_s.shape[0]\n",
    "\n",
    "    C0 = torch.cdist(X_s.reshape((N, -1)), X_t.reshape((N, -1)), p=2).cpu()\n",
    "    C1 = torch.cdist(y_s.reshape((N, -1)), y_t_pred.reshape((N, -1)), p=2).cpu()\n",
    "\n",
    "    alpha = 1  # OT source weight in loss\n",
    "    beta = 1   # OT target weight in loss\n",
    "    C = alpha * C0 + beta * C1\n",
    "\n",
    "    γ = ot.emd(ot.unif(N), ot.unif(N), C.detach().numpy())\n",
    "    γ = torch.from_numpy(γ).float()\n",
    "\n",
    "    loss = torch.sum(γ * C)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f08875c8-73ad-42e3-a518-5384e976a832",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.blstm = nn.LSTM(257, 1024, dropout=0.0, num_layers=2, bidirectional=True, batch_first=True)\n",
    "        self.LReLU = nn.LeakyReLU(0.3)\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.Dropout = nn.Dropout(p=0.0)\n",
    "        self.fc1 = nn.Linear(1024 * 2, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 257)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #  x: clean mag, y: noise mag\n",
    "        output, _ = self.blstm(x)\n",
    "        output = self.fc1(output)\n",
    "        output = self.LReLU(output)\n",
    "        output = self.Dropout(output)\n",
    "        output = self.fc2(output)\n",
    "        output = self.ReLU(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d3ac157-c88d-4620-a2ac-1b3c0411cae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(  # input shape (batch_size, 1, 64, 257)\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,  # input height\n",
    "                out_channels=8,  # n_filters\n",
    "                kernel_size=5,  # filter size\n",
    "                stride=1,  # filter movement/step\n",
    "                padding=2,\n",
    "                # if want same width and length of this image after Conv2d, padding=(kernel_size-1)/2 if stride=1\n",
    "            ),  # output shape (16, 28, 28)\n",
    "            nn.ReLU(),  # activation\n",
    "            nn.MaxPool2d(kernel_size=2),  # choose max value in 2x2 area, output shape (batch_size, 16, 32, 128)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(  # input shape (batch_size, 16, 32, 128)\n",
    "            nn.Conv2d(8, 16, 5, 1, 2),\n",
    "            nn.ReLU(),  # activation\n",
    "            nn.MaxPool2d(2),  # output shape (batch_size, 32, 16, 64)\n",
    "        )\n",
    "        self.out1 = nn.Sequential(\n",
    "            nn.Linear(16 * 16 * 64, 16 * 16),  # fully connected layer, output 10 classes\n",
    "            nn.ReLU()  # activation\n",
    "        )\n",
    "\n",
    "        self.out2 = nn.Sequential(\n",
    "            nn.Linear(16 * 16, 1),  # fully connected layer, output 10 classes\n",
    "            nn.Sigmoid()  # activation\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten the output of conv2 to (batch_size, 32 * 16 * 64)\n",
    "        x = self.out1(x)\n",
    "        output = self.out2(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ce04129-041e-4ce0-b694-4a93bd0fce4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "412e36a6-fa97-463b-97c1-53774d93e239",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7150a4c-1d93-4ac4-aadd-c2810f6c06ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'NUM_EPOCHS' : 150,\n",
    "    'optimizer_g' : torch.optim.Adam(G.parameters(), lr=1e-5),\n",
    "    'optimizer_d' : torch.optim.Adam(D.parameters(), lr=1e-3),\n",
    "    'optimizer_s' : torch.optim.Adam(G.parameters(), lr=1e-4),\n",
    "    'optimizer_ot' : torch.optim.Adam(G.parameters(), lr=1e-5),\n",
    "    'criterion_g' : nn.MSELoss()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75f3f125-5f14-4131-bd8b-c3ada7ff35c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS_TRAIN_MSE_S = []\n",
    "LOSS_VAL_MSE_S = []\n",
    "LOSS_TRAIN_MSE_T = []\n",
    "LOSS_VAL_MSE_T = []\n",
    "TIME = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "212a2012-7768-47a1-8b64-b81237e153cb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 took 186.83 seconds\n",
      "========================================================\n",
      "Epoch 1 S_MSE Train loss: 0.04782 T_MSE Train loss: 0.02975\n",
      "Epoch 1 S_MSE Val loss: 0.04847 T_MSE Val loss: 0.03097\n",
      "\n",
      "\n",
      "Epoch 2 took 189.73 seconds\n",
      "========================================================\n",
      "Epoch 2 S_MSE Train loss: 0.03660 T_MSE Train loss: 0.02342\n",
      "Epoch 2 S_MSE Val loss: 0.03723 T_MSE Val loss: 0.02455\n",
      "\n",
      "\n",
      "Epoch 3 took 197.30 seconds\n",
      "========================================================\n",
      "Epoch 3 S_MSE Train loss: 0.03339 T_MSE Train loss: 0.02012\n",
      "Epoch 3 S_MSE Val loss: 0.03406 T_MSE Val loss: 0.02156\n",
      "\n",
      "\n",
      "Epoch 4 took 201.37 seconds\n",
      "========================================================\n",
      "Epoch 4 S_MSE Train loss: 0.03198 T_MSE Train loss: 0.01784\n",
      "Epoch 4 S_MSE Val loss: 0.03351 T_MSE Val loss: 0.01900\n",
      "\n",
      "\n",
      "Epoch 5 took 202.24 seconds\n",
      "========================================================\n",
      "Epoch 5 S_MSE Train loss: 0.03249 T_MSE Train loss: 0.01657\n",
      "Epoch 5 S_MSE Val loss: 0.03350 T_MSE Val loss: 0.01759\n",
      "\n",
      "\n",
      "Epoch 6 took 202.38 seconds\n",
      "========================================================\n",
      "Epoch 6 S_MSE Train loss: 0.02983 T_MSE Train loss: 0.01587\n",
      "Epoch 6 S_MSE Val loss: 0.03005 T_MSE Val loss: 0.01668\n",
      "\n",
      "\n",
      "Epoch 7 took 203.95 seconds\n",
      "========================================================\n",
      "Epoch 7 S_MSE Train loss: 0.02731 T_MSE Train loss: 0.01482\n",
      "Epoch 7 S_MSE Val loss: 0.02843 T_MSE Val loss: 0.01629\n",
      "\n",
      "\n",
      "Epoch 8 took 204.83 seconds\n",
      "========================================================\n",
      "Epoch 8 S_MSE Train loss: 0.02602 T_MSE Train loss: 0.01481\n",
      "Epoch 8 S_MSE Val loss: 0.02667 T_MSE Val loss: 0.01588\n",
      "\n",
      "\n",
      "Epoch 9 took 204.47 seconds\n",
      "========================================================\n",
      "Epoch 9 S_MSE Train loss: 0.02665 T_MSE Train loss: 0.01367\n",
      "Epoch 9 S_MSE Val loss: 0.02848 T_MSE Val loss: 0.01470\n",
      "\n",
      "\n",
      "Epoch 10 took 205.42 seconds\n",
      "========================================================\n",
      "Epoch 10 S_MSE Train loss: 0.02676 T_MSE Train loss: 0.01368\n",
      "Epoch 10 S_MSE Val loss: 0.02729 T_MSE Val loss: 0.01440\n",
      "\n",
      "\n",
      "Epoch 11 took 205.78 seconds\n",
      "========================================================\n",
      "Epoch 11 S_MSE Train loss: 0.02392 T_MSE Train loss: 0.01288\n",
      "Epoch 11 S_MSE Val loss: 0.02514 T_MSE Val loss: 0.01407\n",
      "\n",
      "\n",
      "Epoch 12 took 206.12 seconds\n",
      "========================================================\n",
      "Epoch 12 S_MSE Train loss: 0.02492 T_MSE Train loss: 0.01370\n",
      "Epoch 12 S_MSE Val loss: 0.02666 T_MSE Val loss: 0.01528\n",
      "\n",
      "\n",
      "Epoch 13 took 207.00 seconds\n",
      "========================================================\n",
      "Epoch 13 S_MSE Train loss: 0.02135 T_MSE Train loss: 0.01233\n",
      "Epoch 13 S_MSE Val loss: 0.02267 T_MSE Val loss: 0.01353\n",
      "\n",
      "\n",
      "Epoch 14 took 206.01 seconds\n",
      "========================================================\n",
      "Epoch 14 S_MSE Train loss: 0.02022 T_MSE Train loss: 0.01233\n",
      "Epoch 14 S_MSE Val loss: 0.02123 T_MSE Val loss: 0.01326\n",
      "\n",
      "\n",
      "Epoch 15 took 206.09 seconds\n",
      "========================================================\n",
      "Epoch 15 S_MSE Train loss: 0.02059 T_MSE Train loss: 0.01172\n",
      "Epoch 15 S_MSE Val loss: 0.02261 T_MSE Val loss: 0.01282\n",
      "\n",
      "\n",
      "Epoch 16 took 206.38 seconds\n",
      "========================================================\n",
      "Epoch 16 S_MSE Train loss: 0.02177 T_MSE Train loss: 0.01171\n",
      "Epoch 16 S_MSE Val loss: 0.02333 T_MSE Val loss: 0.01273\n",
      "\n",
      "\n",
      "Epoch 17 took 214.01 seconds\n",
      "========================================================\n",
      "Epoch 17 S_MSE Train loss: 0.01984 T_MSE Train loss: 0.01140\n",
      "Epoch 17 S_MSE Val loss: 0.02083 T_MSE Val loss: 0.01265\n",
      "\n",
      "\n",
      "Epoch 18 took 205.94 seconds\n",
      "========================================================\n",
      "Epoch 18 S_MSE Train loss: 0.01923 T_MSE Train loss: 0.01094\n",
      "Epoch 18 S_MSE Val loss: 0.02051 T_MSE Val loss: 0.01197\n",
      "\n",
      "\n",
      "Epoch 19 took 203.98 seconds\n",
      "========================================================\n",
      "Epoch 19 S_MSE Train loss: 0.01966 T_MSE Train loss: 0.01115\n",
      "Epoch 19 S_MSE Val loss: 0.02127 T_MSE Val loss: 0.01229\n",
      "\n",
      "\n",
      "Epoch 20 took 205.28 seconds\n",
      "========================================================\n",
      "Epoch 20 S_MSE Train loss: 0.01816 T_MSE Train loss: 0.01074\n",
      "Epoch 20 S_MSE Val loss: 0.01903 T_MSE Val loss: 0.01169\n",
      "\n",
      "\n",
      "Epoch 21 took 205.35 seconds\n",
      "========================================================\n",
      "Epoch 21 S_MSE Train loss: 0.01842 T_MSE Train loss: 0.01051\n",
      "Epoch 21 S_MSE Val loss: 0.01929 T_MSE Val loss: 0.01163\n",
      "\n",
      "\n",
      "Epoch 22 took 297.22 seconds\n",
      "========================================================\n",
      "Epoch 22 S_MSE Train loss: 0.01765 T_MSE Train loss: 0.01020\n",
      "Epoch 22 S_MSE Val loss: 0.01975 T_MSE Val loss: 0.01148\n",
      "\n",
      "\n",
      "Epoch 23 took 311.64 seconds\n",
      "========================================================\n",
      "Epoch 23 S_MSE Train loss: 0.01728 T_MSE Train loss: 0.01015\n",
      "Epoch 23 S_MSE Val loss: 0.01789 T_MSE Val loss: 0.01107\n",
      "\n",
      "\n",
      "Epoch 24 took 310.16 seconds\n",
      "========================================================\n",
      "Epoch 24 S_MSE Train loss: 0.01761 T_MSE Train loss: 0.01005\n",
      "Epoch 24 S_MSE Val loss: 0.01897 T_MSE Val loss: 0.01122\n",
      "\n",
      "\n",
      "Epoch 25 took 308.91 seconds\n",
      "========================================================\n",
      "Epoch 25 S_MSE Train loss: 0.01875 T_MSE Train loss: 0.01024\n",
      "Epoch 25 S_MSE Val loss: 0.01979 T_MSE Val loss: 0.01134\n",
      "\n",
      "\n",
      "Epoch 26 took 307.39 seconds\n",
      "========================================================\n",
      "Epoch 26 S_MSE Train loss: 0.01621 T_MSE Train loss: 0.00956\n",
      "Epoch 26 S_MSE Val loss: 0.01776 T_MSE Val loss: 0.01078\n",
      "\n",
      "\n",
      "Epoch 27 took 306.75 seconds\n",
      "========================================================\n",
      "Epoch 27 S_MSE Train loss: 0.01702 T_MSE Train loss: 0.00976\n",
      "Epoch 27 S_MSE Val loss: 0.01821 T_MSE Val loss: 0.01060\n",
      "\n",
      "\n",
      "Epoch 28 took 305.97 seconds\n",
      "========================================================\n",
      "Epoch 28 S_MSE Train loss: 0.01684 T_MSE Train loss: 0.00944\n",
      "Epoch 28 S_MSE Val loss: 0.01802 T_MSE Val loss: 0.01042\n",
      "\n",
      "\n",
      "Epoch 29 took 306.65 seconds\n",
      "========================================================\n",
      "Epoch 29 S_MSE Train loss: 0.01683 T_MSE Train loss: 0.00943\n",
      "Epoch 29 S_MSE Val loss: 0.01774 T_MSE Val loss: 0.01091\n",
      "\n",
      "\n",
      "Epoch 30 took 306.35 seconds\n",
      "========================================================\n",
      "Epoch 30 S_MSE Train loss: 0.01683 T_MSE Train loss: 0.00945\n",
      "Epoch 30 S_MSE Val loss: 0.01805 T_MSE Val loss: 0.01067\n",
      "\n",
      "\n",
      "Epoch 31 took 306.25 seconds\n",
      "========================================================\n",
      "Epoch 31 S_MSE Train loss: 0.01563 T_MSE Train loss: 0.00902\n",
      "Epoch 31 S_MSE Val loss: 0.01705 T_MSE Val loss: 0.01019\n",
      "\n",
      "\n",
      "Epoch 32 took 306.48 seconds\n",
      "========================================================\n",
      "Epoch 32 S_MSE Train loss: 0.01514 T_MSE Train loss: 0.00889\n",
      "Epoch 32 S_MSE Val loss: 0.01609 T_MSE Val loss: 0.01008\n",
      "\n",
      "\n",
      "Epoch 33 took 308.42 seconds\n",
      "========================================================\n",
      "Epoch 33 S_MSE Train loss: 0.01507 T_MSE Train loss: 0.00901\n",
      "Epoch 33 S_MSE Val loss: 0.01585 T_MSE Val loss: 0.01079\n",
      "\n",
      "\n",
      "Epoch 34 took 308.83 seconds\n",
      "========================================================\n",
      "Epoch 34 S_MSE Train loss: 0.01485 T_MSE Train loss: 0.00884\n",
      "Epoch 34 S_MSE Val loss: 0.01627 T_MSE Val loss: 0.00990\n",
      "\n",
      "\n",
      "Epoch 35 took 309.38 seconds\n",
      "========================================================\n",
      "Epoch 35 S_MSE Train loss: 0.01484 T_MSE Train loss: 0.00862\n",
      "Epoch 35 S_MSE Val loss: 0.01518 T_MSE Val loss: 0.00966\n",
      "\n",
      "\n",
      "Epoch 36 took 308.53 seconds\n",
      "========================================================\n",
      "Epoch 36 S_MSE Train loss: 0.01422 T_MSE Train loss: 0.00836\n",
      "Epoch 36 S_MSE Val loss: 0.01478 T_MSE Val loss: 0.00938\n",
      "\n",
      "\n",
      "Epoch 37 took 309.14 seconds\n",
      "========================================================\n",
      "Epoch 37 S_MSE Train loss: 0.01454 T_MSE Train loss: 0.00840\n",
      "Epoch 37 S_MSE Val loss: 0.01520 T_MSE Val loss: 0.00950\n",
      "\n",
      "\n",
      "Epoch 38 took 308.97 seconds\n",
      "========================================================\n",
      "Epoch 38 S_MSE Train loss: 0.01452 T_MSE Train loss: 0.00856\n",
      "Epoch 38 S_MSE Val loss: 0.01587 T_MSE Val loss: 0.00970\n",
      "\n",
      "\n",
      "Epoch 39 took 309.28 seconds\n",
      "========================================================\n",
      "Epoch 39 S_MSE Train loss: 0.01409 T_MSE Train loss: 0.00860\n",
      "Epoch 39 S_MSE Val loss: 0.01485 T_MSE Val loss: 0.00990\n",
      "\n",
      "\n",
      "Epoch 40 took 296.16 seconds\n",
      "========================================================\n",
      "Epoch 40 S_MSE Train loss: 0.01372 T_MSE Train loss: 0.00827\n",
      "Epoch 40 S_MSE Val loss: 0.01572 T_MSE Val loss: 0.00962\n",
      "\n",
      "\n",
      "Epoch 41 took 192.45 seconds\n",
      "========================================================\n",
      "Epoch 41 S_MSE Train loss: 0.01345 T_MSE Train loss: 0.00845\n",
      "Epoch 41 S_MSE Val loss: 0.01378 T_MSE Val loss: 0.00937\n",
      "\n",
      "\n",
      "Epoch 42 took 201.93 seconds\n",
      "========================================================\n",
      "Epoch 42 S_MSE Train loss: 0.01337 T_MSE Train loss: 0.00823\n",
      "Epoch 42 S_MSE Val loss: 0.01378 T_MSE Val loss: 0.00923\n",
      "\n",
      "\n",
      "Epoch 43 took 203.08 seconds\n",
      "========================================================\n",
      "Epoch 43 S_MSE Train loss: 0.01395 T_MSE Train loss: 0.00809\n",
      "Epoch 43 S_MSE Val loss: 0.01508 T_MSE Val loss: 0.00925\n",
      "\n",
      "\n",
      "Epoch 44 took 204.35 seconds\n",
      "========================================================\n",
      "Epoch 44 S_MSE Train loss: 0.01300 T_MSE Train loss: 0.00791\n",
      "Epoch 44 S_MSE Val loss: 0.01343 T_MSE Val loss: 0.00888\n",
      "\n",
      "\n",
      "Epoch 45 took 204.29 seconds\n",
      "========================================================\n",
      "Epoch 45 S_MSE Train loss: 0.01267 T_MSE Train loss: 0.00777\n",
      "Epoch 45 S_MSE Val loss: 0.01328 T_MSE Val loss: 0.00898\n",
      "\n",
      "\n",
      "Epoch 46 took 204.95 seconds\n",
      "========================================================\n",
      "Epoch 46 S_MSE Train loss: 0.01351 T_MSE Train loss: 0.00781\n",
      "Epoch 46 S_MSE Val loss: 0.01454 T_MSE Val loss: 0.00885\n",
      "\n",
      "\n",
      "Epoch 47 took 205.02 seconds\n",
      "========================================================\n",
      "Epoch 47 S_MSE Train loss: 0.01264 T_MSE Train loss: 0.00781\n",
      "Epoch 47 S_MSE Val loss: 0.01320 T_MSE Val loss: 0.00883\n",
      "\n",
      "\n",
      "Epoch 48 took 206.25 seconds\n",
      "========================================================\n",
      "Epoch 48 S_MSE Train loss: 0.01228 T_MSE Train loss: 0.00759\n",
      "Epoch 48 S_MSE Val loss: 0.01266 T_MSE Val loss: 0.00862\n",
      "\n",
      "\n",
      "Epoch 49 took 204.90 seconds\n",
      "========================================================\n",
      "Epoch 49 S_MSE Train loss: 0.01210 T_MSE Train loss: 0.00756\n",
      "Epoch 49 S_MSE Val loss: 0.01308 T_MSE Val loss: 0.00874\n",
      "\n",
      "\n",
      "Epoch 50 took 205.85 seconds\n",
      "========================================================\n",
      "Epoch 50 S_MSE Train loss: 0.01237 T_MSE Train loss: 0.00736\n",
      "Epoch 50 S_MSE Val loss: 0.01320 T_MSE Val loss: 0.00841\n",
      "\n",
      "\n",
      "Epoch 51 took 203.49 seconds\n",
      "========================================================\n",
      "Epoch 51 S_MSE Train loss: 0.01229 T_MSE Train loss: 0.00741\n",
      "Epoch 51 S_MSE Val loss: 0.01362 T_MSE Val loss: 0.00881\n",
      "\n",
      "\n",
      "Epoch 52 took 204.17 seconds\n",
      "========================================================\n",
      "Epoch 52 S_MSE Train loss: 0.01196 T_MSE Train loss: 0.00733\n",
      "Epoch 52 S_MSE Val loss: 0.01244 T_MSE Val loss: 0.00850\n",
      "\n",
      "\n",
      "Epoch 53 took 205.15 seconds\n",
      "========================================================\n",
      "Epoch 53 S_MSE Train loss: 0.01211 T_MSE Train loss: 0.00736\n",
      "Epoch 53 S_MSE Val loss: 0.01280 T_MSE Val loss: 0.00857\n",
      "\n",
      "\n",
      "Epoch 54 took 204.54 seconds\n",
      "========================================================\n",
      "Epoch 54 S_MSE Train loss: 0.01244 T_MSE Train loss: 0.00721\n",
      "Epoch 54 S_MSE Val loss: 0.01297 T_MSE Val loss: 0.00842\n",
      "\n",
      "\n",
      "Epoch 55 took 203.63 seconds\n",
      "========================================================\n",
      "Epoch 55 S_MSE Train loss: 0.01241 T_MSE Train loss: 0.00728\n",
      "Epoch 55 S_MSE Val loss: 0.01305 T_MSE Val loss: 0.00853\n",
      "\n",
      "\n",
      "Epoch 56 took 203.96 seconds\n",
      "========================================================\n",
      "Epoch 56 S_MSE Train loss: 0.01226 T_MSE Train loss: 0.00736\n",
      "Epoch 56 S_MSE Val loss: 0.01276 T_MSE Val loss: 0.00852\n",
      "\n",
      "\n",
      "Epoch 57 took 203.92 seconds\n",
      "========================================================\n",
      "Epoch 57 S_MSE Train loss: 0.01194 T_MSE Train loss: 0.00693\n",
      "Epoch 57 S_MSE Val loss: 0.01264 T_MSE Val loss: 0.00819\n",
      "\n",
      "\n",
      "Epoch 58 took 204.01 seconds\n",
      "========================================================\n",
      "Epoch 58 S_MSE Train loss: 0.01204 T_MSE Train loss: 0.00708\n",
      "Epoch 58 S_MSE Val loss: 0.01326 T_MSE Val loss: 0.00860\n",
      "\n",
      "\n",
      "Epoch 59 took 203.50 seconds\n",
      "========================================================\n",
      "Epoch 59 S_MSE Train loss: 0.01176 T_MSE Train loss: 0.00705\n",
      "Epoch 59 S_MSE Val loss: 0.01260 T_MSE Val loss: 0.00893\n",
      "\n",
      "\n",
      "Epoch 60 took 203.86 seconds\n",
      "========================================================\n",
      "Epoch 60 S_MSE Train loss: 0.01152 T_MSE Train loss: 0.00708\n",
      "Epoch 60 S_MSE Val loss: 0.01201 T_MSE Val loss: 0.00821\n",
      "\n",
      "\n",
      "Epoch 61 took 203.36 seconds\n",
      "========================================================\n",
      "Epoch 61 S_MSE Train loss: 0.01158 T_MSE Train loss: 0.00700\n",
      "Epoch 61 S_MSE Val loss: 0.01210 T_MSE Val loss: 0.00809\n",
      "\n",
      "\n",
      "Epoch 62 took 203.05 seconds\n",
      "========================================================\n",
      "Epoch 62 S_MSE Train loss: 0.01161 T_MSE Train loss: 0.00678\n",
      "Epoch 62 S_MSE Val loss: 0.01231 T_MSE Val loss: 0.00800\n",
      "\n",
      "\n",
      "Epoch 63 took 204.57 seconds\n",
      "========================================================\n",
      "Epoch 63 S_MSE Train loss: 0.01151 T_MSE Train loss: 0.00679\n",
      "Epoch 63 S_MSE Val loss: 0.01209 T_MSE Val loss: 0.00798\n",
      "\n",
      "\n",
      "Epoch 64 took 206.22 seconds\n",
      "========================================================\n",
      "Epoch 64 S_MSE Train loss: 0.01154 T_MSE Train loss: 0.00665\n",
      "Epoch 64 S_MSE Val loss: 0.01201 T_MSE Val loss: 0.00789\n",
      "\n",
      "\n",
      "Epoch 65 took 206.79 seconds\n",
      "========================================================\n",
      "Epoch 65 S_MSE Train loss: 0.01205 T_MSE Train loss: 0.00692\n",
      "Epoch 65 S_MSE Val loss: 0.01245 T_MSE Val loss: 0.00810\n",
      "\n",
      "\n",
      "Epoch 66 took 205.69 seconds\n",
      "========================================================\n",
      "Epoch 66 S_MSE Train loss: 0.01162 T_MSE Train loss: 0.00672\n",
      "Epoch 66 S_MSE Val loss: 0.01221 T_MSE Val loss: 0.00788\n",
      "\n",
      "\n",
      "Epoch 67 took 205.57 seconds\n",
      "========================================================\n",
      "Epoch 67 S_MSE Train loss: 0.01110 T_MSE Train loss: 0.00652\n",
      "Epoch 67 S_MSE Val loss: 0.01211 T_MSE Val loss: 0.00789\n",
      "\n",
      "\n",
      "Epoch 68 took 205.59 seconds\n",
      "========================================================\n",
      "Epoch 68 S_MSE Train loss: 0.01131 T_MSE Train loss: 0.00660\n",
      "Epoch 68 S_MSE Val loss: 0.01198 T_MSE Val loss: 0.00784\n",
      "\n",
      "\n",
      "Epoch 69 took 205.22 seconds\n",
      "========================================================\n",
      "Epoch 69 S_MSE Train loss: 0.01167 T_MSE Train loss: 0.00665\n",
      "Epoch 69 S_MSE Val loss: 0.01339 T_MSE Val loss: 0.00827\n",
      "\n",
      "\n",
      "Epoch 70 took 204.25 seconds\n",
      "========================================================\n",
      "Epoch 70 S_MSE Train loss: 0.01090 T_MSE Train loss: 0.00633\n",
      "Epoch 70 S_MSE Val loss: 0.01236 T_MSE Val loss: 0.00753\n",
      "\n",
      "\n",
      "Epoch 71 took 203.84 seconds\n",
      "========================================================\n",
      "Epoch 71 S_MSE Train loss: 0.01128 T_MSE Train loss: 0.00630\n",
      "Epoch 71 S_MSE Val loss: 0.01198 T_MSE Val loss: 0.00754\n",
      "\n",
      "\n",
      "Epoch 72 took 205.02 seconds\n",
      "========================================================\n",
      "Epoch 72 S_MSE Train loss: 0.01040 T_MSE Train loss: 0.00633\n",
      "Epoch 72 S_MSE Val loss: 0.01192 T_MSE Val loss: 0.00764\n",
      "\n",
      "\n",
      "Epoch 73 took 205.83 seconds\n",
      "========================================================\n",
      "Epoch 73 S_MSE Train loss: 0.01111 T_MSE Train loss: 0.00625\n",
      "Epoch 73 S_MSE Val loss: 0.01261 T_MSE Val loss: 0.00783\n",
      "\n",
      "\n",
      "Epoch 74 took 205.55 seconds\n",
      "========================================================\n",
      "Epoch 74 S_MSE Train loss: 0.01592 T_MSE Train loss: 0.00671\n",
      "Epoch 74 S_MSE Val loss: 0.01761 T_MSE Val loss: 0.00811\n",
      "\n",
      "\n",
      "Epoch 75 took 205.07 seconds\n",
      "========================================================\n",
      "Epoch 75 S_MSE Train loss: 0.01130 T_MSE Train loss: 0.00625\n",
      "Epoch 75 S_MSE Val loss: 0.01208 T_MSE Val loss: 0.00781\n",
      "\n",
      "\n",
      "Epoch 76 took 205.68 seconds\n",
      "========================================================\n",
      "Epoch 76 S_MSE Train loss: 0.01044 T_MSE Train loss: 0.00614\n",
      "Epoch 76 S_MSE Val loss: 0.01118 T_MSE Val loss: 0.00752\n",
      "\n",
      "\n",
      "Epoch 77 took 205.68 seconds\n",
      "========================================================\n",
      "Epoch 77 S_MSE Train loss: 0.01046 T_MSE Train loss: 0.00609\n",
      "Epoch 77 S_MSE Val loss: 0.01095 T_MSE Val loss: 0.00735\n",
      "\n",
      "\n",
      "Epoch 78 took 204.96 seconds\n",
      "========================================================\n",
      "Epoch 78 S_MSE Train loss: 0.01003 T_MSE Train loss: 0.00597\n",
      "Epoch 78 S_MSE Val loss: 0.01097 T_MSE Val loss: 0.00730\n",
      "\n",
      "\n",
      "Epoch 79 took 203.00 seconds\n",
      "========================================================\n",
      "Epoch 79 S_MSE Train loss: 0.01047 T_MSE Train loss: 0.00609\n",
      "Epoch 79 S_MSE Val loss: 0.01109 T_MSE Val loss: 0.00748\n",
      "\n",
      "\n",
      "Epoch 80 took 204.09 seconds\n",
      "========================================================\n",
      "Epoch 80 S_MSE Train loss: 0.01008 T_MSE Train loss: 0.00608\n",
      "Epoch 80 S_MSE Val loss: 0.01093 T_MSE Val loss: 0.00760\n",
      "\n",
      "\n",
      "Epoch 81 took 203.40 seconds\n",
      "========================================================\n",
      "Epoch 81 S_MSE Train loss: 0.01013 T_MSE Train loss: 0.00594\n",
      "Epoch 81 S_MSE Val loss: 0.01077 T_MSE Val loss: 0.00725\n",
      "\n",
      "\n",
      "Epoch 82 took 203.56 seconds\n",
      "========================================================\n",
      "Epoch 82 S_MSE Train loss: 0.01016 T_MSE Train loss: 0.00587\n",
      "Epoch 82 S_MSE Val loss: 0.01071 T_MSE Val loss: 0.00720\n",
      "\n",
      "\n",
      "Epoch 83 took 203.61 seconds\n",
      "========================================================\n",
      "Epoch 83 S_MSE Train loss: 0.00990 T_MSE Train loss: 0.00588\n",
      "Epoch 83 S_MSE Val loss: 0.01053 T_MSE Val loss: 0.00734\n",
      "\n",
      "\n",
      "Epoch 84 took 203.41 seconds\n",
      "========================================================\n",
      "Epoch 84 S_MSE Train loss: 0.01011 T_MSE Train loss: 0.00579\n",
      "Epoch 84 S_MSE Val loss: 0.01070 T_MSE Val loss: 0.00734\n",
      "\n",
      "\n",
      "Epoch 85 took 203.40 seconds\n",
      "========================================================\n",
      "Epoch 85 S_MSE Train loss: 0.01316 T_MSE Train loss: 0.00729\n",
      "Epoch 85 S_MSE Val loss: 0.01336 T_MSE Val loss: 0.00829\n",
      "\n",
      "\n",
      "Epoch 86 took 203.76 seconds\n",
      "========================================================\n",
      "Epoch 86 S_MSE Train loss: 0.00998 T_MSE Train loss: 0.00590\n",
      "Epoch 86 S_MSE Val loss: 0.01085 T_MSE Val loss: 0.00715\n",
      "\n",
      "\n",
      "Epoch 87 took 204.70 seconds\n",
      "========================================================\n",
      "Epoch 87 S_MSE Train loss: 0.01018 T_MSE Train loss: 0.00591\n",
      "Epoch 87 S_MSE Val loss: 0.01083 T_MSE Val loss: 0.00719\n",
      "\n",
      "\n",
      "Epoch 88 took 205.57 seconds\n",
      "========================================================\n",
      "Epoch 88 S_MSE Train loss: 0.01006 T_MSE Train loss: 0.00567\n",
      "Epoch 88 S_MSE Val loss: 0.01067 T_MSE Val loss: 0.00714\n",
      "\n",
      "\n",
      "Epoch 89 took 205.60 seconds\n",
      "========================================================\n",
      "Epoch 89 S_MSE Train loss: 0.00949 T_MSE Train loss: 0.00562\n",
      "Epoch 89 S_MSE Val loss: 0.01008 T_MSE Val loss: 0.00700\n",
      "\n",
      "\n",
      "Epoch 90 took 205.03 seconds\n",
      "========================================================\n",
      "Epoch 90 S_MSE Train loss: 0.00964 T_MSE Train loss: 0.00568\n",
      "Epoch 90 S_MSE Val loss: 0.01012 T_MSE Val loss: 0.00721\n",
      "\n",
      "\n",
      "Epoch 91 took 204.82 seconds\n",
      "========================================================\n",
      "Epoch 91 S_MSE Train loss: 0.01007 T_MSE Train loss: 0.00564\n",
      "Epoch 91 S_MSE Val loss: 0.01113 T_MSE Val loss: 0.00757\n",
      "\n",
      "\n",
      "Epoch 92 took 205.50 seconds\n",
      "========================================================\n",
      "Epoch 92 S_MSE Train loss: 0.00965 T_MSE Train loss: 0.00560\n",
      "Epoch 92 S_MSE Val loss: 0.01018 T_MSE Val loss: 0.00709\n",
      "\n",
      "\n",
      "Epoch 93 took 205.09 seconds\n",
      "========================================================\n",
      "Epoch 93 S_MSE Train loss: 0.00982 T_MSE Train loss: 0.00556\n",
      "Epoch 93 S_MSE Val loss: 0.01171 T_MSE Val loss: 0.00766\n",
      "\n",
      "\n",
      "Epoch 94 took 205.21 seconds\n",
      "========================================================\n",
      "Epoch 94 S_MSE Train loss: 0.00976 T_MSE Train loss: 0.00555\n",
      "Epoch 94 S_MSE Val loss: 0.01034 T_MSE Val loss: 0.00711\n",
      "\n",
      "\n",
      "Epoch 95 took 204.47 seconds\n",
      "========================================================\n",
      "Epoch 95 S_MSE Train loss: 0.00942 T_MSE Train loss: 0.00543\n",
      "Epoch 95 S_MSE Val loss: 0.01021 T_MSE Val loss: 0.00691\n",
      "\n",
      "\n",
      "Epoch 96 took 205.05 seconds\n",
      "========================================================\n",
      "Epoch 96 S_MSE Train loss: 0.00956 T_MSE Train loss: 0.00539\n",
      "Epoch 96 S_MSE Val loss: 0.01003 T_MSE Val loss: 0.00674\n",
      "\n",
      "\n",
      "Epoch 97 took 204.20 seconds\n",
      "========================================================\n",
      "Epoch 97 S_MSE Train loss: 0.01028 T_MSE Train loss: 0.00553\n",
      "Epoch 97 S_MSE Val loss: 0.01124 T_MSE Val loss: 0.00701\n",
      "\n",
      "\n",
      "Epoch 98 took 204.21 seconds\n",
      "========================================================\n",
      "Epoch 98 S_MSE Train loss: 0.00990 T_MSE Train loss: 0.00541\n",
      "Epoch 98 S_MSE Val loss: 0.01036 T_MSE Val loss: 0.00679\n",
      "\n",
      "\n",
      "Epoch 99 took 204.27 seconds\n",
      "========================================================\n",
      "Epoch 99 S_MSE Train loss: 0.00964 T_MSE Train loss: 0.00542\n",
      "Epoch 99 S_MSE Val loss: 0.01019 T_MSE Val loss: 0.00708\n",
      "\n",
      "\n",
      "Epoch 100 took 204.96 seconds\n",
      "========================================================\n",
      "Epoch 100 S_MSE Train loss: 0.00962 T_MSE Train loss: 0.00545\n",
      "Epoch 100 S_MSE Val loss: 0.01056 T_MSE Val loss: 0.00693\n",
      "\n",
      "\n",
      "Epoch 101 took 204.79 seconds\n",
      "========================================================\n",
      "Epoch 101 S_MSE Train loss: 0.00983 T_MSE Train loss: 0.00539\n",
      "Epoch 101 S_MSE Val loss: 0.01100 T_MSE Val loss: 0.00728\n",
      "\n",
      "\n",
      "Epoch 102 took 204.70 seconds\n",
      "========================================================\n",
      "Epoch 102 S_MSE Train loss: 0.00927 T_MSE Train loss: 0.00527\n",
      "Epoch 102 S_MSE Val loss: 0.01004 T_MSE Val loss: 0.00701\n",
      "\n",
      "\n",
      "Epoch 103 took 203.80 seconds\n",
      "========================================================\n",
      "Epoch 103 S_MSE Train loss: 0.00963 T_MSE Train loss: 0.00525\n",
      "Epoch 103 S_MSE Val loss: 0.01043 T_MSE Val loss: 0.00673\n",
      "\n",
      "\n",
      "Epoch 104 took 204.69 seconds\n",
      "========================================================\n",
      "Epoch 104 S_MSE Train loss: 0.00908 T_MSE Train loss: 0.00525\n",
      "Epoch 104 S_MSE Val loss: 0.00998 T_MSE Val loss: 0.00675\n",
      "\n",
      "\n",
      "Epoch 105 took 204.55 seconds\n",
      "========================================================\n",
      "Epoch 105 S_MSE Train loss: 0.00954 T_MSE Train loss: 0.00530\n",
      "Epoch 105 S_MSE Val loss: 0.01054 T_MSE Val loss: 0.00712\n",
      "\n",
      "\n",
      "Epoch 106 took 204.65 seconds\n",
      "========================================================\n",
      "Epoch 106 S_MSE Train loss: 0.00928 T_MSE Train loss: 0.00522\n",
      "Epoch 106 S_MSE Val loss: 0.01032 T_MSE Val loss: 0.00695\n",
      "\n",
      "\n",
      "Epoch 107 took 203.47 seconds\n",
      "========================================================\n",
      "Epoch 107 S_MSE Train loss: 0.00897 T_MSE Train loss: 0.00513\n",
      "Epoch 107 S_MSE Val loss: 0.00963 T_MSE Val loss: 0.00696\n",
      "\n",
      "\n",
      "Epoch 108 took 204.23 seconds\n",
      "========================================================\n",
      "Epoch 108 S_MSE Train loss: 0.00881 T_MSE Train loss: 0.00509\n",
      "Epoch 108 S_MSE Val loss: 0.00940 T_MSE Val loss: 0.00651\n",
      "\n",
      "\n",
      "Epoch 109 took 204.88 seconds\n",
      "========================================================\n",
      "Epoch 109 S_MSE Train loss: 0.00878 T_MSE Train loss: 0.00534\n",
      "Epoch 109 S_MSE Val loss: 0.00943 T_MSE Val loss: 0.00677\n",
      "\n",
      "\n",
      "Epoch 110 took 204.14 seconds\n",
      "========================================================\n",
      "Epoch 110 S_MSE Train loss: 0.00898 T_MSE Train loss: 0.00511\n",
      "Epoch 110 S_MSE Val loss: 0.00962 T_MSE Val loss: 0.00694\n",
      "\n",
      "\n",
      "Epoch 111 took 203.11 seconds\n",
      "========================================================\n",
      "Epoch 111 S_MSE Train loss: 0.01006 T_MSE Train loss: 0.00559\n",
      "Epoch 111 S_MSE Val loss: 0.01117 T_MSE Val loss: 0.00716\n",
      "\n",
      "\n",
      "Epoch 112 took 204.37 seconds\n",
      "========================================================\n",
      "Epoch 112 S_MSE Train loss: 0.00930 T_MSE Train loss: 0.00519\n",
      "Epoch 112 S_MSE Val loss: 0.01025 T_MSE Val loss: 0.00679\n",
      "\n",
      "\n",
      "Epoch 113 took 204.05 seconds\n",
      "========================================================\n",
      "Epoch 113 S_MSE Train loss: 0.00971 T_MSE Train loss: 0.00528\n",
      "Epoch 113 S_MSE Val loss: 0.01063 T_MSE Val loss: 0.00699\n",
      "\n",
      "\n",
      "Epoch 114 took 204.63 seconds\n",
      "========================================================\n",
      "Epoch 114 S_MSE Train loss: 0.00923 T_MSE Train loss: 0.00496\n",
      "Epoch 114 S_MSE Val loss: 0.01001 T_MSE Val loss: 0.00643\n",
      "\n",
      "\n",
      "Epoch 115 took 203.72 seconds\n",
      "========================================================\n",
      "Epoch 115 S_MSE Train loss: 0.00930 T_MSE Train loss: 0.00496\n",
      "Epoch 115 S_MSE Val loss: 0.00996 T_MSE Val loss: 0.00652\n",
      "\n",
      "\n",
      "Epoch 116 took 204.14 seconds\n",
      "========================================================\n",
      "Epoch 116 S_MSE Train loss: 0.00872 T_MSE Train loss: 0.00497\n",
      "Epoch 116 S_MSE Val loss: 0.01034 T_MSE Val loss: 0.00662\n",
      "\n",
      "\n",
      "Epoch 117 took 204.51 seconds\n",
      "========================================================\n",
      "Epoch 117 S_MSE Train loss: 0.00873 T_MSE Train loss: 0.00489\n",
      "Epoch 117 S_MSE Val loss: 0.00941 T_MSE Val loss: 0.00642\n",
      "\n",
      "\n",
      "Epoch 118 took 204.27 seconds\n",
      "========================================================\n",
      "Epoch 118 S_MSE Train loss: 0.00894 T_MSE Train loss: 0.00498\n",
      "Epoch 118 S_MSE Val loss: 0.00961 T_MSE Val loss: 0.00663\n",
      "\n",
      "\n",
      "Epoch 119 took 203.75 seconds\n",
      "========================================================\n",
      "Epoch 119 S_MSE Train loss: 0.00936 T_MSE Train loss: 0.00486\n",
      "Epoch 119 S_MSE Val loss: 0.00996 T_MSE Val loss: 0.00643\n",
      "\n",
      "\n",
      "Epoch 120 took 203.87 seconds\n",
      "========================================================\n",
      "Epoch 120 S_MSE Train loss: 0.00879 T_MSE Train loss: 0.00487\n",
      "Epoch 120 S_MSE Val loss: 0.00960 T_MSE Val loss: 0.00676\n",
      "\n",
      "\n",
      "Epoch 121 took 204.00 seconds\n",
      "========================================================\n",
      "Epoch 121 S_MSE Train loss: 0.00879 T_MSE Train loss: 0.00482\n",
      "Epoch 121 S_MSE Val loss: 0.00994 T_MSE Val loss: 0.00676\n",
      "\n",
      "\n",
      "Epoch 122 took 204.08 seconds\n",
      "========================================================\n",
      "Epoch 122 S_MSE Train loss: 0.00840 T_MSE Train loss: 0.00484\n",
      "Epoch 122 S_MSE Val loss: 0.01020 T_MSE Val loss: 0.00707\n",
      "\n",
      "\n",
      "Epoch 123 took 202.23 seconds\n",
      "========================================================\n",
      "Epoch 123 S_MSE Train loss: 0.00872 T_MSE Train loss: 0.00476\n",
      "Epoch 123 S_MSE Val loss: 0.00933 T_MSE Val loss: 0.00650\n",
      "\n",
      "\n",
      "Epoch 124 took 202.11 seconds\n",
      "========================================================\n",
      "Epoch 124 S_MSE Train loss: 0.01153 T_MSE Train loss: 0.00513\n",
      "Epoch 124 S_MSE Val loss: 0.01255 T_MSE Val loss: 0.00691\n",
      "\n",
      "\n",
      "Epoch 125 took 203.73 seconds\n",
      "========================================================\n",
      "Epoch 125 S_MSE Train loss: 0.00977 T_MSE Train loss: 0.00534\n",
      "Epoch 125 S_MSE Val loss: 0.01061 T_MSE Val loss: 0.00706\n",
      "\n",
      "\n",
      "Epoch 126 took 203.38 seconds\n",
      "========================================================\n",
      "Epoch 126 S_MSE Train loss: 0.00900 T_MSE Train loss: 0.00484\n",
      "Epoch 126 S_MSE Val loss: 0.01009 T_MSE Val loss: 0.00657\n",
      "\n",
      "\n",
      "Epoch 127 took 203.33 seconds\n",
      "========================================================\n",
      "Epoch 127 S_MSE Train loss: 0.00875 T_MSE Train loss: 0.00472\n",
      "Epoch 127 S_MSE Val loss: 0.01009 T_MSE Val loss: 0.00650\n",
      "\n",
      "\n",
      "Epoch 128 took 203.15 seconds\n",
      "========================================================\n",
      "Epoch 128 S_MSE Train loss: 0.00848 T_MSE Train loss: 0.00472\n",
      "Epoch 128 S_MSE Val loss: 0.01353 T_MSE Val loss: 0.00685\n",
      "\n",
      "\n",
      "Epoch 129 took 202.02 seconds\n",
      "========================================================\n",
      "Epoch 129 S_MSE Train loss: 0.00887 T_MSE Train loss: 0.00464\n",
      "Epoch 129 S_MSE Val loss: 0.01020 T_MSE Val loss: 0.00649\n",
      "\n",
      "\n",
      "Epoch 130 took 202.74 seconds\n",
      "========================================================\n",
      "Epoch 130 S_MSE Train loss: 0.00898 T_MSE Train loss: 0.00467\n",
      "Epoch 130 S_MSE Val loss: 0.00960 T_MSE Val loss: 0.00641\n",
      "\n",
      "\n",
      "Epoch 131 took 202.36 seconds\n",
      "========================================================\n",
      "Epoch 131 S_MSE Train loss: 0.00851 T_MSE Train loss: 0.00462\n",
      "Epoch 131 S_MSE Val loss: 0.00921 T_MSE Val loss: 0.00641\n",
      "\n",
      "\n",
      "Epoch 132 took 203.20 seconds\n",
      "========================================================\n",
      "Epoch 132 S_MSE Train loss: 0.00826 T_MSE Train loss: 0.00469\n",
      "Epoch 132 S_MSE Val loss: 0.00889 T_MSE Val loss: 0.00639\n",
      "\n",
      "\n",
      "Epoch 133 took 203.40 seconds\n",
      "========================================================\n",
      "Epoch 133 S_MSE Train loss: 0.00862 T_MSE Train loss: 0.00466\n",
      "Epoch 133 S_MSE Val loss: 0.00933 T_MSE Val loss: 0.00633\n",
      "\n",
      "\n",
      "Epoch 134 took 203.77 seconds\n",
      "========================================================\n",
      "Epoch 134 S_MSE Train loss: 0.00898 T_MSE Train loss: 0.00474\n",
      "Epoch 134 S_MSE Val loss: 0.00999 T_MSE Val loss: 0.00667\n",
      "\n",
      "\n",
      "Epoch 135 took 203.14 seconds\n",
      "========================================================\n",
      "Epoch 135 S_MSE Train loss: 0.00833 T_MSE Train loss: 0.00456\n",
      "Epoch 135 S_MSE Val loss: 0.00917 T_MSE Val loss: 0.00655\n",
      "\n",
      "\n",
      "Epoch 136 took 203.55 seconds\n",
      "========================================================\n",
      "Epoch 136 S_MSE Train loss: 0.00839 T_MSE Train loss: 0.00453\n",
      "Epoch 136 S_MSE Val loss: 0.00966 T_MSE Val loss: 0.00630\n",
      "\n",
      "\n",
      "Epoch 137 took 203.32 seconds\n",
      "========================================================\n",
      "Epoch 137 S_MSE Train loss: 0.00824 T_MSE Train loss: 0.00450\n",
      "Epoch 137 S_MSE Val loss: 0.00924 T_MSE Val loss: 0.00636\n",
      "\n",
      "\n",
      "Epoch 138 took 203.52 seconds\n",
      "========================================================\n",
      "Epoch 138 S_MSE Train loss: 0.00845 T_MSE Train loss: 0.00455\n",
      "Epoch 138 S_MSE Val loss: 0.00957 T_MSE Val loss: 0.00636\n",
      "\n",
      "\n",
      "Epoch 139 took 203.12 seconds\n",
      "========================================================\n",
      "Epoch 139 S_MSE Train loss: 0.00844 T_MSE Train loss: 0.00453\n",
      "Epoch 139 S_MSE Val loss: 0.00927 T_MSE Val loss: 0.00625\n",
      "\n",
      "\n",
      "Epoch 140 took 202.43 seconds\n",
      "========================================================\n",
      "Epoch 140 S_MSE Train loss: 0.00804 T_MSE Train loss: 0.00453\n",
      "Epoch 140 S_MSE Val loss: 0.00871 T_MSE Val loss: 0.00644\n",
      "\n",
      "\n",
      "Epoch 141 took 202.52 seconds\n",
      "========================================================\n",
      "Epoch 141 S_MSE Train loss: 0.00814 T_MSE Train loss: 0.00448\n",
      "Epoch 141 S_MSE Val loss: 0.00908 T_MSE Val loss: 0.00642\n",
      "\n",
      "\n",
      "Epoch 142 took 201.64 seconds\n",
      "========================================================\n",
      "Epoch 142 S_MSE Train loss: 0.00796 T_MSE Train loss: 0.00445\n",
      "Epoch 142 S_MSE Val loss: 0.00876 T_MSE Val loss: 0.00662\n",
      "\n",
      "\n",
      "Epoch 143 took 201.47 seconds\n",
      "========================================================\n",
      "Epoch 143 S_MSE Train loss: 0.00829 T_MSE Train loss: 0.00444\n",
      "Epoch 143 S_MSE Val loss: 0.00917 T_MSE Val loss: 0.00646\n",
      "\n",
      "\n",
      "Epoch 144 took 201.91 seconds\n",
      "========================================================\n",
      "Epoch 144 S_MSE Train loss: 0.00823 T_MSE Train loss: 0.00441\n",
      "Epoch 144 S_MSE Val loss: 0.00914 T_MSE Val loss: 0.00627\n",
      "\n",
      "\n",
      "Epoch 145 took 201.74 seconds\n",
      "========================================================\n",
      "Epoch 145 S_MSE Train loss: 0.00832 T_MSE Train loss: 0.00437\n",
      "Epoch 145 S_MSE Val loss: 0.00933 T_MSE Val loss: 0.00641\n",
      "\n",
      "\n",
      "Epoch 146 took 202.77 seconds\n",
      "========================================================\n",
      "Epoch 146 S_MSE Train loss: 0.00811 T_MSE Train loss: 0.00437\n",
      "Epoch 146 S_MSE Val loss: 0.00910 T_MSE Val loss: 0.00629\n",
      "\n",
      "\n",
      "Epoch 147 took 202.46 seconds\n",
      "========================================================\n",
      "Epoch 147 S_MSE Train loss: 0.00790 T_MSE Train loss: 0.00439\n",
      "Epoch 147 S_MSE Val loss: 0.00870 T_MSE Val loss: 0.00630\n",
      "\n",
      "\n",
      "Epoch 148 took 203.25 seconds\n",
      "========================================================\n",
      "Epoch 148 S_MSE Train loss: 0.00812 T_MSE Train loss: 0.00438\n",
      "Epoch 148 S_MSE Val loss: 0.00886 T_MSE Val loss: 0.00631\n",
      "\n",
      "\n",
      "Epoch 149 took 203.10 seconds\n",
      "========================================================\n",
      "Epoch 149 S_MSE Train loss: 0.00816 T_MSE Train loss: 0.00441\n",
      "Epoch 149 S_MSE Val loss: 0.00898 T_MSE Val loss: 0.00627\n",
      "\n",
      "\n",
      "Epoch 150 took 201.12 seconds\n",
      "========================================================\n",
      "Epoch 150 S_MSE Train loss: 0.00814 T_MSE Train loss: 0.00433\n",
      "Epoch 150 S_MSE Val loss: 0.00893 T_MSE Val loss: 0.00621\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training with OT Loss\n",
    "for epoch in range(hyperparameters['NUM_EPOCHS']):\n",
    "    start_time = time.time()\n",
    "    epoch_OT_loss = 0.0\n",
    "    epoch_S_loss = 0.0\n",
    "    epoch_D_loss = 0.0\n",
    "    epoch_G_loss = 0.0\n",
    "    #Train NN\n",
    "    D.train()\n",
    "    G.train()\n",
    "    Count = 0\n",
    "    for batch_idx, (clean_stft_mag_features_s, noise_stft_mag_features_s, _, noise_stft_mag_features_t) in enumerate(custom_dataloader_train):\n",
    "        #load features in CUDA\n",
    "        y_s = clean_stft_mag_features_s.to(device)\n",
    "        X_s = noise_stft_mag_features_s.to(device)\n",
    "        #y_t = clean_stft_mag_features_t.to(device)\n",
    "        X_t = noise_stft_mag_features_t.to(device)\n",
    "        if (batch_idx % 13 == 0):\n",
    "            hyperparameters['optimizer_d'].zero_grad()\n",
    "            loss_d = -torch.mean(D(y_s)) + torch.mean(D(G(X_t).detach()))\n",
    "            loss_d.backward()\n",
    "            hyperparameters['optimizer_d'].step()\n",
    "            for p in D.parameters():\n",
    "                p.data.clamp_(-0.001, 0.001)\n",
    "            epoch_D_loss += loss_d.item()\n",
    "        if (batch_idx % 6 == 0):\n",
    "            hyperparameters['optimizer_g'].zero_grad()\n",
    "            loss_g = -torch.mean(D(G(X_t)))\n",
    "            loss_g.backward()\n",
    "            hyperparameters['optimizer_g'].step()\n",
    "            epoch_G_loss += loss_g.item()\n",
    "        if (batch_idx % 2 == 0):\n",
    "            loss_s = hyperparameters['criterion_g'](G(X_t), y_s)\n",
    "            hyperparameters['optimizer_s'].zero_grad()\n",
    "            loss_s.backward()\n",
    "            hyperparameters['optimizer_s'].step()\n",
    "            epoch_S_loss += loss_s.item()\n",
    "        ot_loss = OT_loss(X_s, X_t, y_s, G(X_s))\n",
    "        hyperparameters['optimizer_ot'].zero_grad()\n",
    "        ot_loss.backward()\n",
    "        hyperparameters['optimizer_ot'].step()\n",
    "        epoch_OT_loss += ot_loss.item()\n",
    "        # Remove features from CUDA\n",
    "        del y_s, X_s, X_t\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    #Eval NN\n",
    "    G.eval()\n",
    "    with torch.no_grad():\n",
    "        running_mse_loss_train_s = 0.0\n",
    "        running_mse_loss_train_t = 0.0\n",
    "        for batch_idx, (clean_stft_mag_features_s, noise_stft_mag_features_s, clean_stft_mag_features_t, noise_stft_mag_features_t) in enumerate(custom_dataloader_train):\n",
    "            #load features in CUDA\n",
    "            y_s = clean_stft_mag_features_s.to(device)\n",
    "            X_s = noise_stft_mag_features_s.to(device)\n",
    "            y_t = clean_stft_mag_features_t.to(device)\n",
    "            X_t = noise_stft_mag_features_t.to(device)\n",
    "            outputs_s = G(X_s)\n",
    "            outputs_t = G(X_t)\n",
    "            mse_train_s = hyperparameters['criterion_g'](outputs_s, y_s)\n",
    "            mse_train_t = hyperparameters['criterion_g'](outputs_t, y_t)\n",
    "            running_mse_loss_train_s += mse_train_s.item()\n",
    "            running_mse_loss_train_t += mse_train_t.item()\n",
    "            del y_s, X_s, y_t, X_t, outputs_s, outputs_t\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()    \n",
    "    #Eval NN\n",
    "    G.eval()\n",
    "    with torch.no_grad():\n",
    "        running_mse_loss_val_s = 0.0\n",
    "        running_mse_loss_val_t = 0.0\n",
    "        for batch_idx, (clean_stft_mag_features_s, noise_stft_mag_features_s, clean_stft_mag_features_t, noise_stft_mag_features_t) in enumerate(custom_dataloader_val):\n",
    "            #load features in CUDA\n",
    "            y_s = clean_stft_mag_features_s.to(device)\n",
    "            X_s = noise_stft_mag_features_s.to(device)\n",
    "            y_t = clean_stft_mag_features_t.to(device)\n",
    "            X_t = noise_stft_mag_features_t.to(device)\n",
    "            outputs_s = G(X_s)\n",
    "            outputs_t = G(X_t)\n",
    "            mse_val_s = hyperparameters['criterion_g'](outputs_s, y_s)\n",
    "            mse_val_t = hyperparameters['criterion_g'](outputs_t, y_t)\n",
    "            running_mse_loss_val_s += mse_val_s.item()\n",
    "            running_mse_loss_val_t += mse_val_t.item()\n",
    "            del y_s, X_s, y_t, X_t, outputs_s, outputs_t\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "    # Scores for train and validation\n",
    "    epoch_time = time.time() - start_time\n",
    "    TIME.append(epoch_time)\n",
    "    print(f\"Epoch {epoch+1} took {epoch_time:.2f} seconds\\n========================================================\")\n",
    "    print('Epoch %d S_MSE Train loss: %.5f T_MSE Train loss: %.5f' % \n",
    "          (epoch + 1, \n",
    "           running_mse_loss_train_s/len(custom_dataloader_train),\n",
    "           running_mse_loss_train_t/len(custom_dataloader_train)))\n",
    "    print('Epoch %d S_MSE Val loss: %.5f T_MSE Val loss: %.5f' % \n",
    "          (epoch + 1, \n",
    "           running_mse_loss_val_s/len(custom_dataloader_val),\n",
    "           running_mse_loss_val_t/len(custom_dataloader_val)))\n",
    "    \n",
    "    LOSS_TRAIN_MSE_S.append(running_mse_loss_train_s/len(custom_dataloader_train))\n",
    "    LOSS_VAL_MSE_S.append(running_mse_loss_val_s/len(custom_dataloader_val))\n",
    "    LOSS_TRAIN_MSE_T.append(running_mse_loss_train_t/len(custom_dataloader_train))\n",
    "    LOSS_VAL_MSE_T.append(running_mse_loss_val_t/len(custom_dataloader_val))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9da3caff-6185-4eab-b789-1c3194ab84e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(G.state_dict(), 'models_regression/G_model_NOISE_DA_WITH_OT_LOSS_SourceTarget_ALL_NOISES_dB_Generator_FINAL.pt')\n",
    "torch.save(D.state_dict(), 'models_regression/D_model_NOISE_DA_WITH_OT_LOSS_SourceTarget_ALL_NOISES_dB_Discriminador_FINAL.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09d8d33b-eb54-4b63-87be-cb629f9c6b35",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.047819566623378204, 0.03660100558381371, 0.0333938226573357, 0.03198343392524148, 0.03249464376571299, 0.029833060657965534, 0.02731453431691943, 0.026015028130544834, 0.026654032927726497, 0.026758673694767372, 0.02392238998362998, 0.024915155070293852, 0.021354595930980786, 0.020218397764598623, 0.020594897421718647, 0.021767329886367842, 0.019840442825954, 0.019234575151207568, 0.019657525132183266, 0.018160430874925953, 0.018415987526825748, 0.017651044016218736, 0.017275169002627877, 0.017611990878436744, 0.018747999814158978, 0.0162090191680218, 0.017016337437069966, 0.016835643909871578, 0.016834169872762525, 0.01682635845945162, 0.015627570407261617, 0.015136551514987936, 0.015074099527531061, 0.014845793002418109, 0.014840879317122597, 0.01422232947069682, 0.01454078739884646, 0.014516612741814441, 0.014085417073376539, 0.01372166202567956, 0.01345377296851087, 0.013366491612078263, 0.013948168499911782, 0.012995826733932524, 0.012670524207045552, 0.01351346360018649, 0.012638858660366856, 0.012275722730873512, 0.01209530099725523, 0.012367144903997663, 0.012287747185332935, 0.011961249994630573, 0.012111432091215579, 0.012443303737100684, 0.01240819358431241, 0.012263149739157002, 0.011944263942633607, 0.012041284730110098, 0.011758930499845442, 0.011520769826660887, 0.011582816084220755, 0.011607519335293469, 0.011509937586403694, 0.011536898287091436, 0.012045054473452457, 0.011619524157629544, 0.01110269223582469, 0.011305881165234106, 0.0116694523647678, 0.010897181845982285, 0.011275964935834543, 0.010402098172852974, 0.011107669008441833, 0.015918285112452357, 0.011301746255397295, 0.010438571768194562, 0.010457789212340066, 0.010034200874305949, 0.01047200702146447, 0.010081553397340183, 0.010125197116050776, 0.010159271076603346, 0.009901589989865652, 0.010106054111188199, 0.013162545734593849, 0.009975659715778688, 0.010181067454699082, 0.010064563815708921, 0.00948609076753384, 0.009636120530267973, 0.010070600998861826, 0.009651430348372635, 0.009822483319288292, 0.009757038655870852, 0.0094184821408962, 0.009563491836252833, 0.010276998678895356, 0.0098980525704179, 0.009640889696213378, 0.009621751453617917, 0.009832111357499947, 0.009274164590389789, 0.009631802549022211, 0.00907857418099434, 0.009541443130392738, 0.009279650616451722, 0.008969794299469149, 0.008809605611394308, 0.008779691911826865, 0.00898463633341887, 0.010055791435982375, 0.009303213916953747, 0.00971307625461902, 0.009233687027991443, 0.00930052759133655, 0.008717658326906316, 0.008734543275620257, 0.008935624389212672, 0.009359836637065942, 0.008790202865361417, 0.00878690167901521, 0.008401993801034674, 0.008723605447644446, 0.011534513142586005, 0.009766204110464128, 0.009001675345582747, 0.008745553746756887, 0.008478869871870309, 0.00887449029167848, 0.008983684660663375, 0.008505842335396955, 0.008256509291602909, 0.00862114375825001, 0.008981882001688125, 0.00833340729938961, 0.008388325593536016, 0.00824146276778158, 0.008454651671594806, 0.008444247698849616, 0.008041750465543205, 0.008135860361268414, 0.007963516980315707, 0.008287989851400382, 0.008234863331885774, 0.008324391824905486, 0.008108932317524147, 0.007901187917655882, 0.008124428844282857, 0.008158304493780397, 0.008138605409121815]\n",
      "150\n",
      "[0.04847365353376635, 0.03722778755810953, 0.03405946685421851, 0.03351031604313081, 0.0335004210111595, 0.03005479142490414, 0.028429851176277283, 0.02666796112973844, 0.02847768306251495, 0.027292606030260364, 0.025144338337404114, 0.026659418378145464, 0.022671711841417898, 0.021234828978776932, 0.022605631320226575, 0.023331238377478816, 0.02083180478263286, 0.020510447992672844, 0.021265436444551714, 0.019032003418091806, 0.019286717618665388, 0.019749206971497305, 0.0178883342042325, 0.01896914353053416, 0.01978759647857758, 0.017757851930875933, 0.018210296338844682, 0.018018274958575924, 0.017735091819157525, 0.0180542103405441, 0.017046104996435104, 0.016091929508313056, 0.015846323852817857, 0.016268723524145542, 0.015177673404856075, 0.01478100279646535, 0.015201545230323268, 0.0158698417306427, 0.014852875362961523, 0.015724687776978937, 0.013779674326219866, 0.013784508280936749, 0.015075858261796736, 0.01342818372312092, 0.013277099497856633, 0.01454280184641961, 0.013204181116194494, 0.012664520211758153, 0.013075512743765307, 0.013195975022690912, 0.013623689601738606, 0.01243752809692054, 0.012800759545737696, 0.012968960548600843, 0.013052673289372076, 0.012764795435472362, 0.012635543401683531, 0.01325917394170838, 0.01260452032569916, 0.012011067040504949, 0.012104859335287925, 0.012308017951586554, 0.012088314119365907, 0.012013086222953374, 0.012445624992852249, 0.012208051679115142, 0.012112118063434478, 0.011977560517768706, 0.013392707633395349, 0.012359686376106354, 0.01198154665349472, 0.011921487448196258, 0.012608686281788734, 0.017607606616952726, 0.012078023245257715, 0.011177746639136345, 0.010953040125088827, 0.010967193712149896, 0.011089917212244003, 0.01093008756757744, 0.010769333690404892, 0.010709028721096055, 0.010534431364747786, 0.010699638616173499, 0.013356758777292506, 0.01085033779963851, 0.010834876297702712, 0.010667094990851419, 0.010084565007878889, 0.010117534855981507, 0.01113290117392617, 0.010180576922251813, 0.011705936082909185, 0.010338890336213572, 0.010209463822144655, 0.010026824200570944, 0.011242944146356276, 0.01036283431289297, 0.010189589158061051, 0.010563663131887875, 0.010999557201660449, 0.010038430457033457, 0.010433236978227092, 0.009976632232146879, 0.010536414150509142, 0.010317026742643887, 0.009629712635350804, 0.009400571989376218, 0.00943357809897392, 0.009617788687108024, 0.011166653205310144, 0.010253656415208694, 0.010634245501170236, 0.010005786354022642, 0.00995868254422901, 0.01033607575922243, 0.009408560972059927, 0.009610476422934763, 0.009956025375774311, 0.009599704385524797, 0.009940892848516665, 0.010204095363376601, 0.009325671820871292, 0.012545297132624735, 0.010606184977317048, 0.010089337900881805, 0.010085732946472784, 0.013529988606610605, 0.01019508020591832, 0.00959591786827772, 0.009205683104453547, 0.008892656777114157, 0.009325052987063123, 0.009992958767519843, 0.009167458666788956, 0.009658430733026998, 0.009239605883316647, 0.009573437483800996, 0.009274871629332342, 0.008714230933917625, 0.00907632655974838, 0.008757986008159576, 0.009168192909489717, 0.009141536219225775, 0.009325108851396268, 0.009095166779814227, 0.008697651206485687, 0.008860402898262105, 0.008975548970122491, 0.008930665065324115]\n",
      "150\n",
      "[0.02975125948139349, 0.023422170125198465, 0.0201159916786837, 0.01784454255184832, 0.01656576546942111, 0.01587306002170599, 0.014817995191434351, 0.01481282472328729, 0.013670863528900287, 0.013677409924410222, 0.012878724099958645, 0.013699505423360011, 0.012325350504305934, 0.012331421874432253, 0.011717381718873727, 0.011712917760640634, 0.01139876720722483, 0.010935427699269367, 0.011153648947874288, 0.010739286633662316, 0.010514045055924343, 0.01019632164780207, 0.01015200958579403, 0.010052954304587691, 0.010237872888169745, 0.009562367236852144, 0.009757885854190388, 0.009438343980044377, 0.009428279726773877, 0.009450163406578182, 0.009023792810832001, 0.008892522835243149, 0.009013326177295266, 0.008836017733737201, 0.008621526927053303, 0.008361993296885816, 0.008397603646202498, 0.008559795892836541, 0.008599139730438214, 0.008265536442837295, 0.008446963163515349, 0.008232021925407423, 0.008090636882727513, 0.007906493466707463, 0.007768272929003134, 0.0078123442949961965, 0.007806756300851703, 0.007592191600765125, 0.007557203306448685, 0.00736048660914124, 0.0074125035262877954, 0.007333027729138481, 0.007361587896129163, 0.007212333373349754, 0.007277764125625376, 0.007363038951484095, 0.006926123686760915, 0.0070844809916026956, 0.007048168544414915, 0.007078384602943263, 0.006995390723028979, 0.006781612718556107, 0.00678882870173567, 0.006649564793176886, 0.0069204993632097705, 0.0067179345236230295, 0.0065159963173870026, 0.0066008913847265625, 0.006647607270518646, 0.006327011976625864, 0.006303209604053688, 0.006328368050774105, 0.006245821732010285, 0.006707765698088568, 0.006252302743197114, 0.006135505723900029, 0.006091494015378862, 0.005971904366644991, 0.006087327383834274, 0.006081267893940461, 0.0059387255381761475, 0.0058682151256176105, 0.005875456737777015, 0.005794204691299746, 0.007288386350685433, 0.005903305581716054, 0.005912381408250883, 0.0056676230659442285, 0.005622733437197925, 0.005683680211718218, 0.0056414642711678726, 0.005601642953678027, 0.005564630984430428, 0.005548600956578465, 0.005434298733280611, 0.005394330962381067, 0.005528946841495628, 0.005414024425842682, 0.005422611067368954, 0.005451878268631692, 0.005392685972795892, 0.00526668515424205, 0.005254289542301362, 0.005247711340951569, 0.005302072011576104, 0.005221162998986219, 0.005126914750988118, 0.005085010940459471, 0.005336870693703409, 0.005109612673458432, 0.005587195245815175, 0.005189632818869808, 0.005281955036999924, 0.00495675831109661, 0.004963538135957567, 0.004966440407105354, 0.0048943536097825575, 0.004977816974102449, 0.004860828888892126, 0.004872569726279178, 0.004823688813876517, 0.00484484202862412, 0.004762253639906519, 0.00513032718538111, 0.005342135771310755, 0.004841387561451988, 0.004715931480515655, 0.004715248656717418, 0.004635223132722518, 0.004669916487353689, 0.00461981282373812, 0.0046944756279973425, 0.004656004502258005, 0.004742727485806251, 0.004562570476716682, 0.004527466887209032, 0.004501535534710005, 0.004548686440102756, 0.004531293691668127, 0.004534546590112776, 0.004482837296051889, 0.004452957117249172, 0.004438974767406823, 0.0044074752143028525, 0.004374954967853464, 0.004367366114224331, 0.004393056430266935, 0.004381237661137301, 0.004414948831437579, 0.004332258841492424]\n",
      "150\n",
      "[0.03096936426816448, 0.02455302201692135, 0.021557944976995067, 0.018995956788139957, 0.01758590635032423, 0.01668046127944704, 0.016292806143962568, 0.015881719308034066, 0.014696081229034931, 0.01440265980519114, 0.014071446843445301, 0.015279708580384332, 0.013527297835436559, 0.013259319393264671, 0.012817669269298353, 0.012726727212148329, 0.012653839023363206, 0.01197411414356001, 0.012292972555564295, 0.011687328017527056, 0.011628959448106827, 0.011478447775927282, 0.011074271263374437, 0.011222047551024345, 0.01134280130387314, 0.010778333601211347, 0.010598389942559504, 0.010423823620282834, 0.010911819404892383, 0.010673489932331347, 0.010190138863700052, 0.010077122478715835, 0.010794000820286812, 0.009903945960104465, 0.009659309090385515, 0.009383858106429539, 0.009501516819000244, 0.009704101079654309, 0.009897152622861246, 0.00961733621455008, 0.009373473934829235, 0.00922834740260676, 0.009249149461186702, 0.008879410358326088, 0.008981461531572765, 0.008846300999603925, 0.008828285270400586, 0.008623638836246344, 0.00873602977803638, 0.008411531727160177, 0.008814501050379007, 0.008498421975321347, 0.008573259166892498, 0.008424931386065098, 0.008534921723748408, 0.00852105868679862, 0.008188075671393064, 0.008601175230597296, 0.008932269148288234, 0.00821096000952586, 0.00809065222499832, 0.007996201740517732, 0.00797683868797556, 0.007889532123602206, 0.008102075106674624, 0.00788195330589529, 0.007890397036868719, 0.007841651508164021, 0.008274666048706539, 0.007529347361395916, 0.007537696262701384, 0.007642540585009321, 0.007826366146365482, 0.008114411465583308, 0.007805563614613587, 0.007522464819973515, 0.007348839028347884, 0.007298933221928535, 0.0074827581373674255, 0.007603583617075797, 0.007246297363552355, 0.007195436170384768, 0.0073369741890459295, 0.007342063776788211, 0.008291640589314123, 0.007150583111891343, 0.007186887785792351, 0.007140089021695237, 0.006998870401613174, 0.007207473259299032, 0.0075658805097543426, 0.007092561407555495, 0.007656387338835386, 0.007106223864661109, 0.006907782697629544, 0.0067370846987732, 0.0070096529990194304, 0.006788712109424054, 0.00707789387313589, 0.006934479119316224, 0.007276709972610397, 0.007006634146936478, 0.006727731929370953, 0.006752900644055297, 0.007119349834899749, 0.006946563420276488, 0.006958186356050353, 0.006510912722367193, 0.006771234975707146, 0.00693648774176836, 0.007159477249989586, 0.006792375502446967, 0.006986798192825048, 0.006431458919729677, 0.006522823658381258, 0.006617759684881856, 0.006421987853583789, 0.006633896110278945, 0.0064322202271150965, 0.00676039980363942, 0.006763714606002454, 0.007068211663394205, 0.006496120244264603, 0.006910839658831397, 0.007057011187557251, 0.006571046001608333, 0.006500989393962007, 0.006845661213681582, 0.00648795340150114, 0.0064114974361033205, 0.006406744356237112, 0.006392397354507158, 0.006330116006034997, 0.006669728941614589, 0.006545675648075919, 0.006298654487416629, 0.006358641229810253, 0.00636006806105856, 0.00624989092935838, 0.0064443785757307085, 0.00641653320241359, 0.006620246331177412, 0.006461874897321386, 0.006268539526049168, 0.006411250260087752, 0.006287602497444999, 0.006299496463109408, 0.006312632843131019, 0.006269788767601694, 0.006207297120483652]\n",
      "150\n",
      "[186.83200454711914, 189.73074889183044, 197.30429601669312, 201.36960530281067, 202.2380986213684, 202.37675642967224, 203.9511902332306, 204.82855701446533, 204.4696831703186, 205.42437744140625, 205.77897000312805, 206.11900162696838, 207.00077104568481, 206.0080223083496, 206.090345621109, 206.37678813934326, 214.014568567276, 205.93773746490479, 203.98376369476318, 205.28271055221558, 205.35423946380615, 297.2220821380615, 311.64262294769287, 310.1614589691162, 308.90840697288513, 307.3851683139801, 306.748902797699, 305.97011280059814, 306.6544373035431, 306.354740858078, 306.2462840080261, 306.4787788391113, 308.4207479953766, 308.82619643211365, 309.3776435852051, 308.53034019470215, 309.1426887512207, 308.9729404449463, 309.2842571735382, 296.16344952583313, 192.44664001464844, 201.9263961315155, 203.07779717445374, 204.34562826156616, 204.294100522995, 204.9510428905487, 205.01842641830444, 206.25044345855713, 204.90066862106323, 205.84661626815796, 203.48681616783142, 204.16904997825623, 205.1525115966797, 204.53623223304749, 203.6280336380005, 203.95776534080505, 203.91535687446594, 204.00744891166687, 203.50026559829712, 203.8588252067566, 203.3621850013733, 203.04899859428406, 204.56918740272522, 206.22130250930786, 206.79489612579346, 205.69026684761047, 205.56659984588623, 205.5871877670288, 205.2157106399536, 204.25470066070557, 203.843980550766, 205.01899194717407, 205.8261260986328, 205.54728055000305, 205.06899976730347, 205.68346500396729, 205.68374133110046, 204.95846366882324, 202.99862837791443, 204.09448790550232, 203.40498089790344, 203.56350541114807, 203.60896515846252, 203.4058120250702, 203.39655828475952, 203.75787448883057, 204.6967043876648, 205.57017636299133, 205.60161685943604, 205.02808356285095, 204.81894326210022, 205.5009479522705, 205.08851599693298, 205.21200275421143, 204.4713282585144, 205.0509147644043, 204.19986581802368, 204.2103009223938, 204.2724962234497, 204.96486163139343, 204.79319739341736, 204.6983301639557, 203.79768896102905, 204.69120693206787, 204.54712986946106, 204.65383124351501, 203.4743869304657, 204.23498487472534, 204.87757968902588, 204.1360161304474, 203.11484456062317, 204.3669514656067, 204.0513105392456, 204.63441705703735, 203.7183518409729, 204.13662719726562, 204.5107443332672, 204.27473378181458, 203.75154066085815, 203.8659405708313, 203.99711346626282, 204.08496284484863, 202.23481225967407, 202.11446475982666, 203.7289524078369, 203.37527084350586, 203.33277654647827, 203.15364742279053, 202.02028059959412, 202.7409553527832, 202.36252331733704, 203.19638013839722, 203.398770570755, 203.7736575603485, 203.13900089263916, 203.54598236083984, 203.31582045555115, 203.52267169952393, 203.1150243282318, 202.43215894699097, 202.5206801891327, 201.6395184993744, 201.46636962890625, 201.91266632080078, 201.73753452301025, 202.77451419830322, 202.45679926872253, 203.2479648590088, 203.09594011306763, 201.12370991706848]\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "util.generate_pkl('stastics/', LOSS_TRAIN_MSE_S, 'LOSS_TRAIN_MSE_S')\n",
    "util.generate_pkl('stastics/', LOSS_VAL_MSE_S, 'LOSS_VAL_MSE_S')\n",
    "util.generate_pkl('stastics/', LOSS_TRAIN_MSE_T, 'LOSS_TRAIN_MSE_T')\n",
    "util.generate_pkl('stastics/', LOSS_VAL_MSE_T, 'LOSS_VAL_MSE_T')\n",
    "util.generate_pkl('stastics/', TIME, 'TIME_DAOT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c7b242-ec22-4953-9e4d-f02df594b7f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
